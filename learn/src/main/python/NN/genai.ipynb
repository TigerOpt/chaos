{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成式人工智能的构建模块\n",
    "## 文本预处理\n",
    "### Tokenization\n",
    "Word tokenization\n",
    "将文本拆分成单个单词，eg：“Generative AI is charming.”变成[\"Generative\", \"AI\", \"is\", \"fascinating\", \".\"]\n",
    "Subword tokenization\n",
    "将单词拆分成更小的单元，对处理位置或罕见单词特别有用。eg：“unhappiness”变成[\"un\", \"happiness\"]\n",
    "Character tokenization\n",
    "将文本拆分为单个字符。eg：[\"G\", \"e\", \"n\", \"e\", \"r\", \"a\", \"t\", \"i\", \"v\", \"e\", ...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Generative', 'AI', 'is', 'charming', '.']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "一个基本的单词标记器。此示例将把一个句子拆分为单词和标点符号，演示标记化如何构造原始文本。\n",
    "'''\n",
    "def simple_tokenize(text):\n",
    "    tokens = []\n",
    "    current_word = \"\"\n",
    "    for char in text:\n",
    "        if char.isalnum():\n",
    "            current_word += char\n",
    "        else:\n",
    "            if current_word != \"\":\n",
    "                tokens.append(current_word)\n",
    "                current_word = \"\"\n",
    "            if char.strip() != \"\":\n",
    "                tokens.append(char)\n",
    "    if current_word != \"\":\n",
    "        tokens.append(current_word)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "sentence = \"Generative AI is charming.\"\n",
    "tokens = simple_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取\n",
    "一种基于规则的过程，通过删除常见的前缀或后缀来截断单词。它快速且计算简单，因此在文档分类和搜索引擎索引等任务中很受欢迎。eg：“cats”和“cat”等单词合并为“cat”\n",
    "![词干提取](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['runn', 'happi', 'tri', 'faster', 'cat']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "一个基本的词干提取器来删除常见后缀。\n",
    "'''\n",
    "def simple_stem(word):\n",
    "    suffixes = [\"ing\", \"ly\", \"ed\", \"ious\", \"ies\", \"ive\", \"es\", \"s\", \"ment\"]\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]  # Remove the matched suffix.\n",
    "    return word\n",
    "\n",
    "# Example usage\n",
    "words = [\"running\", \"happily\", \"tried\", \"faster\", \"cats\"]\n",
    "stemmed_words = [simple_stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词形还原\n",
    "将单词映射到其基本形式或词典形式（ 词根 ）。与词干提取不同，词形还原通常需要了解单词的词性，并且可能依赖于形态分析器或词汇数据库。\n",
    "![词形还原](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'happy', 'run', 'good', 'fast', 'cat']\n"
     ]
    }
   ],
   "source": [
    "'''一个基本的词形还原器'''\n",
    "def simple_lemmatize(word):\n",
    "    # A minimal dictionary for known irregular forms.\n",
    "    irregular_lemmas = {\n",
    "        \"running\": \"run\",\n",
    "        \"happily\": \"happy\",\n",
    "        \"ran\": \"run\",\n",
    "        \"better\": \"good\",\n",
    "        \"faster\": \"fast\",\n",
    "        \"cats\": \"cat\",\n",
    "        \"dogs\": \"dog\",\n",
    "        \"are\": \"be\",\n",
    "        \"is\": \"be\",\n",
    "        \"have\": \"have\"\n",
    "    }\n",
    "    return irregular_lemmas.get(word, word)\n",
    "\n",
    "# Example usage\n",
    "words = [\"running\", \"happily\", \"ran\", \"better\", \"faster\", \"cats\"]\n",
    "lemmatized_words = [simple_lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question：词干提取和词形还原哪个更好？\n",
    "Answer：取决于具体的需求，当需要速度并且可以容忍一些不准确性时，选择词干提取 。它适合大型应用程序，例如为搜索引擎索引文档。当需要准确性和语义正确性时，选择词形还原 。它非常适合那些需要理解单词精确含义的任务。\n",
    "eg：Elasticsearch 等搜索引擎经常使用词干分析来快速索引文档，确保“run”、“running”和“ran”等查询检索到相关结果。相比之下，需要细致入微的语言理解的应用程序（例如客户评论中的情绪分析）更能从词形还原中获益，因为它可以准确捕捉不同词形所表达的情绪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其它预处理技术\n",
    "Lowercasing\n",
    "通过将所有单词转换为小写来标准化文本，从而减少词汇量并提高模型效率。\n",
    "Removing stop words\n",
    "删除停用词会消除常见但语义上不重要的词（例如“the”、“is”），从而使模型能够专注于有意义的内容。\n",
    "Stripping punctuation\n",
    "删除标点符号会删除不必要的符号，这些符号会增加复杂性，而不会对文本分类等任务的意义有所帮助。\n",
    "Handling special characters and numbers\n",
    "处理特殊字符和数字可确保仅保留相关元素，具体取决于任务（例如，保留数字以进行情感分析，但删除它们以进行一般文本处理）。\n",
    "Handling contractions \n",
    "处理缩写意味着扩展缩写（例如，将“don't”扩展为“do not”）可以提高理解。\n",
    "Correcting misspellings \n",
    "纠正拼写错误意味着自动修复拼写错误以确保数据集的一致性。\n",
    "Dealing with abbreviations and acronyms\n",
    "处理缩写和首字母缩略词意味着扩展或标准化缩写（例如，将“AI”改为“人工智能”）可以提高清晰度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Apple released the iPhone! I didn't know that Apple's announcement would shock everyone. Don't you think it's amazing?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Lowercasing:\n",
      "apple released the iphone! i didn't know that apple's announcement would shock everyone. don't you think it's amazing?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Tokenization:\n",
      "['apple', 'released', 'the', 'iphone!', 'i', \"didn't\", 'know', 'that', \"apple's\", 'announcement', 'would', 'shock', 'everyone.', \"don't\", 'you', 'think', \"it's\", 'amazing?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Removing Punctuation:\n",
      "['apple', 'released', 'the', 'iphone', 'i', \"didn't\", 'know', 'that', \"apple's\", 'announcement', 'would', 'shock', 'everyone', \"don't\", 'you', 'think', \"it's\", 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Removing Stop Words:\n",
      "['apple', 'released', 'iphone', 'i', \"didn't\", 'know', \"apple's\", 'announcement', 'shock', 'everyone', \"don't\", 'think', \"it's\", 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Expanding Contractions:\n",
      "['apple', 'released', 'iphone', 'i', 'did', 'not', 'know', 'apple', 'has', 'announcement', 'shock', 'everyone', 'do', 'not', 'think', 'it', 'is', 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Handling Numbers:\n",
      "['apple', 'released', 'iphone', 'i', 'did', 'not', 'know', 'apple', 'has', 'announcement', 'shock', 'everyone', 'do', 'not', 'think', 'it', 'is', 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Correcting Misspellings:\n",
      "['apple', 'released', 'iphone', 'i', 'did', 'not', 'know', 'apple', 'has', 'announcement', 'shock', 'everyone', 'do', 'not', 'think', 'it', 'is', 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Expanding Abbreviations:\n",
      "['apple', 'released', 'iphone', 'i', 'did', 'not', 'know', 'apple', 'has', 'announcement', 'shock', 'everyone', 'do', 'not', 'think', 'it', 'is', 'amazing']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final Preprocessed Tokens:\n",
      "['apple', 'released', 'iphone', 'i', 'did', 'not', 'know', 'apple', 'has', 'announcement', 'shock', 'everyone', 'do', 'not', 'think', 'it', 'is', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing various cases\n",
    "text = \"Apple released the iPhone! I didn't know that Apple's announcement would shock everyone. Don't you think it's amazing?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 1. Lowercasing: Convert all text to lowercase\n",
    "lower_text = text.lower()\n",
    "print(\"After Lowercasing:\")\n",
    "print(lower_text)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 2. Tokenization: Split text into words (this simple approach splits on whitespace)\n",
    "tokens = lower_text.split()\n",
    "print(\"After Tokenization:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 3. Stripping Punctuation: Remove punctuation from each token\n",
    "# Define a set of punctuation characters\n",
    "punctuations = '.,!?\\'\":;()'\n",
    "tokens = [token.strip(punctuations) for token in tokens]\n",
    "print(\"After Removing Punctuation:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 4. Removing Stop Words: Filter out common, semantically insignificant words\n",
    "stop_words = ['the', 'is', 'at', 'on', 'and', 'a', 'an', 'of', 'that', 'would', 'you', 'it']\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"After Removing Stop Words:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 5. Expanding Contractions: Replace contractions with their expanded forms\n",
    "# Note: This is a simple dictionary for demonstration\n",
    "contractions = {\n",
    "    \"didn't\": \"did not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"apple's\": \"apple has\"\n",
    "}\n",
    "\n",
    "expanded_tokens = []\n",
    "for token in tokens:\n",
    "    if token in contractions:\n",
    "        # Split the expanded form to keep tokens consistent\n",
    "        expanded_tokens.extend(contractions[token].split())\n",
    "    else:\n",
    "        expanded_tokens.append(token)\n",
    "tokens = expanded_tokens\n",
    "print(\"After Expanding Contractions:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 6. Handling Special Characters and Numbers:\n",
    "# For this example, remove tokens that are purely numeric.\n",
    "tokens = [token for token in tokens if not token.isdigit()]\n",
    "print(\"After Handling Numbers:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 7. Correcting Misspellings:\n",
    "# A very basic approach using a predefined dictionary of common corrections.\n",
    "corrections = {\n",
    "    \"iphon\": \"iphone\",  # Example: if a typo occurred\n",
    "    # add more common misspellings as needed\n",
    "}\n",
    "tokens = [corrections.get(token, token) for token in tokens]\n",
    "print(\"After Correcting Misspellings:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 8. Dealing with Abbreviations and Acronyms:\n",
    "# Expand or standardize abbreviations using a simple mapping.\n",
    "abbreviations = {\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    # add additional abbreviation mappings as needed\n",
    "}\n",
    "tokens = [abbreviations.get(token, token) for token in tokens]\n",
    "print(\"After Expanding Abbreviations:\")\n",
    "print(tokens)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Final preprocessed tokens\n",
    "print(\"Final Preprocessed Tokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "### BoW（Bag of Words）\n",
    "率先脱离了基于规则的指令，而是依赖于对单词出现次数进行计数。譬如两个句子“I love cats”和“I hate dogs”，我们将这两个句子分词，然后得出一个词汇表[\"I\", \"love\", \"cats\", \"hate\", \"dogs\"],接下来计算每次词汇在句子中出现的次数，比对词汇表，“I love cats”得出向量[1,1,1,0,0],“I hate dogs”得出向量[1,0,0,1,1]\n",
    "![BoW示例](image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['cats' 'dogs' 'hate' 'i' 'love']\n",
      "Vectors:\n",
      " [[1 0 0 1 1]\n",
      " [0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\"I love cats\", \"I hate dogs\"]\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')  # Adjusted pattern to include single characters\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Vectors:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以上示例可以发现，BoW方式忽略了单词的出现顺序，对需要理解语义的场景并不适合，但是类似垃圾邮件分类这种场景，效率很高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency–Inverse Document Frequency)\n",
    "通过根据单词的重要性对单词进行加权来改进BoW方法。\n",
    "$$\n",
    "TF(w) = \\frac{Count of w in a document}{Total number of words in the document}\n",
    "IDF(w) = log(\\frac{Total number of documents}{Number of documents containing w})\n",
    "TF-IDF(w) = TF(w) * IDF(w)\n",
    "$$\n",
    "eg：假设文档中“recipe”的TF为0.05，整个语料库中“recipe”的IDF为2，TF-IDF为TF-IDF(\"recipe\")=0.05×2=0.1\n",
    "TF-IDF 会突出显示文档中重要但独特的单词。像“recipe”这样的单词可能具有较高的 TF-IDF 分数，因为它在特定文档中很常见，但在整个语料库中并不常见。像“the”这样的常用词将具有较低的分数，因为它的 IDF 很低，即使它的 TF 很高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 TF-IDF:\n",
      "  brown: 0.1111\n",
      "  dog: 0.0791\n",
      "  fox: 0.1111\n",
      "  jumps: 0.1562\n",
      "  lazy: 0.0791\n",
      "  over: 0.0791\n",
      "  quick: 0.1562\n",
      "  the: 0.2222\n",
      "\n",
      "Document 2 TF-IDF:\n",
      "  dog: 0.1018\n",
      "  jump: 0.2008\n",
      "  lazy: 0.1018\n",
      "  never: 0.2008\n",
      "  over: 0.1018\n",
      "  quickly: 0.2008\n",
      "  the: 0.1429\n",
      "\n",
      "Document 3 TF-IDF:\n",
      "  a: 0.3123\n",
      "  brown: 0.1111\n",
      "  dog: 0.0791\n",
      "  fast: 0.1562\n",
      "  fox: 0.1111\n",
      "  lazy: 0.0791\n",
      "  leaps: 0.1562\n",
      "  over: 0.0791\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"A fast brown fox leaps over a lazy dog\"\n",
    "]\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Simple tokenizer that lowercases and splits on whitespace.\n",
    "    Removes punctuation for simplicity.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    punctuations = '.,!?;:()[]{}\\'\"'\n",
    "    for p in punctuations:\n",
    "        document = document.replace(p, '')\n",
    "    # Lowercase and split\n",
    "    tokens = document.lower().split()\n",
    "    return tokens\n",
    "\n",
    "def compute_tf(doc_tokens):\n",
    "    \"\"\"\n",
    "    Computes term frequency for a single document.\n",
    "    Returns a dictionary of term frequencies.\n",
    "    \"\"\"\n",
    "    tf = {}\n",
    "    for term in doc_tokens:\n",
    "        tf[term] = tf.get(term, 0) + 1\n",
    "    # Optionally, normalize TF by the total number of terms in the document\n",
    "    total_terms = len(doc_tokens)\n",
    "    for term in tf:\n",
    "        tf[term] = tf[term] / total_terms\n",
    "    return tf\n",
    "\n",
    "def compute_df(documents_tokens):\n",
    "    \"\"\"\n",
    "    Computes document frequency for all terms in the corpus.\n",
    "    Returns a dictionary of document frequencies.\n",
    "    \"\"\"\n",
    "    df = {}\n",
    "    for tokens in documents_tokens:\n",
    "        unique_terms = set(tokens)\n",
    "        for term in unique_terms:\n",
    "            df[term] = df.get(term, 0) + 1\n",
    "    return df\n",
    "\n",
    "def compute_idf(df, total_docs):\n",
    "    \"\"\"\n",
    "    Computes inverse document frequency for all terms.\n",
    "    Returns a dictionary of IDF scores.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    for term, freq in df.items():\n",
    "        idf[term] = math.log(total_docs / (1 + freq)) + 1  # Adding 1 to avoid division by zero\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF for a single document.\n",
    "    Returns a dictionary of TF-IDF scores.\n",
    "    \"\"\"\n",
    "    tf_idf = {}\n",
    "    for term, tf_value in tf.items():\n",
    "        tf_idf[term] = tf_value * idf.get(term, 0)\n",
    "    return tf_idf\n",
    "\n",
    "def main(documents):\n",
    "    # Step 1: Tokenize all documents\n",
    "    documents_tokens = [tokenize(doc) for doc in documents]\n",
    "    \n",
    "    # Step 2: Compute TF for each document\n",
    "    tfs = [compute_tf(tokens) for tokens in documents_tokens]\n",
    "    \n",
    "    # Step 3: Compute DF across all documents\n",
    "    df = compute_df(documents_tokens)\n",
    "    \n",
    "    # Step 4: Compute IDF for all terms\n",
    "    total_docs = len(documents)\n",
    "    idf = compute_idf(df, total_docs)\n",
    "    \n",
    "    # Step 5: Compute TF-IDF for each document\n",
    "    tf_idfs = [compute_tf_idf(tf, idf) for tf in tfs]\n",
    "    \n",
    "    # (Optional) Collect all unique terms for creating a TF-IDF matrix\n",
    "    all_terms = sorted(df.keys())\n",
    "    \n",
    "    # Display TF-IDF scores\n",
    "    for i, tf_idf in enumerate(tf_idfs):\n",
    "        print(f\"\\nDocument {i+1} TF-IDF:\")\n",
    "        for term in all_terms:\n",
    "            score = tf_idf.get(term, 0)\n",
    "            if score > 0:\n",
    "                print(f\"  {term}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram模型\n",
    "给定一个单词序列，模型通过查看文本语料库中单词组合（或“gram”）的频率来估计下一个单词的概率。这就是n-gram背后的理念，基于该理念来解决BoW和TF-IDF牺牲了的顺序的问题。\n",
    "$$\n",
    "P(\\w_i|\\w_i-1) = \\frac{Count(\\w_i-1, \\w_i)}{Count(\\w_i-1)}   # 2-Gram\n",
    "P(\\w_i|\\w_i-2,\\w_i-1) = \\frac{Count(\\w_i-2, \\w_i-1, \\w_i)}{Count(\\w_i-2, \\w_i-1)}  # 3-Gram\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram Probability Matrix:\n",
      "\n",
      "Word     | amazing  | are      | i        | language | love     | models   | natural  | processing\n",
      "---------------------------------------------------------------------------\n",
      "amazing  | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0     \n",
      "are      | 1.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0     \n",
      "i        | 0.0      | 0.0      | 0.0      | 0.0      | 1.0      | 0.0      | 0.0      | 0.0     \n",
      "language | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.5      | 0.0      | 0.5     \n",
      "love     | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 1.0      | 0.0     \n",
      "models   | 0.0      | 1.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0     \n",
      "natural  | 0.0      | 0.0      | 0.0      | 1.0      | 0.0      | 0.0      | 0.0      | 0.0     \n",
      "processing | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0      | 0.0     \n"
     ]
    }
   ],
   "source": [
    "# Toy dataset\n",
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language models are amazing\"\n",
    "]\n",
    "\n",
    "# Function to generate bigrams\n",
    "def generate_bigrams(sentence):\n",
    "    words = sentence.lower().split()  # Tokenization (lowercase + split)\n",
    "    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "    return bigrams, words  # Return bigrams and word list\n",
    "\n",
    "# Collect all words and bigrams\n",
    "all_bigrams = []\n",
    "all_words = set()  # To store unique words\n",
    "\n",
    "for sentence in sentences:\n",
    "    bigrams, words = generate_bigrams(sentence)\n",
    "    all_bigrams.extend(bigrams)\n",
    "    all_words.update(words)\n",
    "\n",
    "# Sort words for consistent ordering\n",
    "unique_words = sorted(all_words)\n",
    "\n",
    "# Create bigram frequency matrix\n",
    "bigram_matrix = {word: {w: 0 for w in unique_words} for word in unique_words}\n",
    "\n",
    "# Count occurrences of bigrams\n",
    "for bigram in all_bigrams:\n",
    "    first, second = bigram\n",
    "    bigram_matrix[first][second] += 1\n",
    "\n",
    "# Convert frequency matrix to probability matrix\n",
    "for word in unique_words:\n",
    "    total_bigrams = sum(bigram_matrix[word].values())  # Total transitions from this word\n",
    "    if total_bigrams > 0:\n",
    "        for next_word in unique_words:\n",
    "            bigram_matrix[word][next_word] /= total_bigrams  # Normalize to get probabilities\n",
    "\n",
    "# Display bigram probability matrix in a well-formatted table\n",
    "print(\"\\nBigram Probability Matrix:\\n\")\n",
    "\n",
    "# Print header row\n",
    "header = [\"Word\"] + unique_words\n",
    "col_width = 8  # Set a fixed column width for better alignment\n",
    "\n",
    "# Print table header\n",
    "print(f\"{header[0]:<{col_width}}\", end=\" | \")\n",
    "print(\" | \".join(f\"{w:<{col_width}}\" for w in header[1:]))\n",
    "print(\"-\" * (col_width * (len(unique_words) + 1) + 3))\n",
    "\n",
    "# Print each row with bigram probabilities\n",
    "for word in unique_words:\n",
    "    row_values = [f\"{bigram_matrix[word][w]:.1f}\" for w in unique_words]\n",
    "    print(f\"{word:<{col_width}}\", end=\" | \")\n",
    "    print(\" | \".join(f\"{val:<{col_width}}\" for val in row_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram模型也有有其自身的局限性和挑战。用固定窗口（例如 3 个单词）捕捉“上下文”就像试图用牙刷画出日落一样——虽然可行，但非常有限。罕见的单词组合表现不佳，因为 n-gram 严重依赖于在训练期间观察所有可能的序列。像“Generative AI rocks”这样的三元组可能永远不会出现在训练语料库中，因此其概率实际上为零。此外，上下文越长（n 越高），所需的数据就越多。模型在计算上变得昂贵，并且对于语言中的大词汇量或长期依赖性来说不切实际。这些缺点为一项关键的进步奠定了基础：词嵌入。与固定窗口模型不同，词嵌入（例如 Word2Vec 生成的词嵌入）允许模型通过将单词映射到连续向量空间来学习语义关系。这一发展通过捕获局部上下文和更广泛的语义含义解决了 n-gram 的局限性，有效地弥合了简单的基于计数的模型与更复杂的神经网络方法之间的差距。最终，这些创新为基于神经网络的方法铺平了道路，这些方法可以更好地概括并捕捉固定 n-gram 窗口之外的含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化语言\n",
    "词向量——一种将单词表示为连续高维空间中的向量的方法。与基于频率的方法不同，词向量通过将单词放置在共享空间中来捕捉单词之间的关系，其中相似的单词最终会更接近在一起。\n",
    "### Word2Vec\n",
    "Word2Vec 通过训练神经网络来预测缺失单词 (CBOW) 或其周围上下文 (Skip-Gram)，从而学习词向量，捕获单词在低维向量空间中一起出现的频率。\n",
    "#### CBOW\n",
    "根据周围的上下文预测序列中的中心词。例如，在句子“The cat sat on the ___”中，模型使用上下文词 [“The”、“cat”、“sat”、“on”] 来预测中心词“mat”。\n",
    "![CBOW](image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      " - I like deep learning\n",
      " - I like NLP\n",
      " - I enjoy flying\n",
      "\n",
      "Tokenized Sentences:\n",
      " - ['i', 'like', 'deep', 'learning']\n",
      " - ['i', 'like', 'nlp']\n",
      " - ['i', 'enjoy', 'flying']\n",
      "\n",
      "Vocabulary (word to index mapping):\n",
      " learning: 0\n",
      " i: 1\n",
      " enjoy: 2\n",
      " nlp: 3\n",
      " flying: 4\n",
      " deep: 5\n",
      " like: 6\n",
      "\n",
      "Training Pairs (context word indices, center word index):\n",
      " Context: ['like'], Center: i (1)\n",
      " Context: ['i', 'deep'], Center: like (6)\n",
      " Context: ['like', 'learning'], Center: deep (5)\n",
      " Context: ['deep'], Center: learning (0)\n",
      " Context: ['like'], Center: i (1)\n",
      " Context: ['i', 'nlp'], Center: like (6)\n",
      " Context: ['like'], Center: nlp (3)\n",
      " Context: ['enjoy'], Center: i (1)\n",
      " Context: ['i', 'flying'], Center: enjoy (2)\n",
      " Context: ['enjoy'], Center: flying (4)\n",
      "\n",
      "Starting CBOW training...\n",
      "\n",
      "Epoch 10/100 - Average Loss: 1.9969\n",
      "Epoch 20/100 - Average Loss: 1.8481\n",
      "Epoch 30/100 - Average Loss: 1.7568\n",
      "Epoch 40/100 - Average Loss: 1.6807\n",
      "Epoch 50/100 - Average Loss: 1.6051\n",
      "Epoch 60/100 - Average Loss: 1.5254\n",
      "Epoch 70/100 - Average Loss: 1.4413\n",
      "Epoch 80/100 - Average Loss: 1.3546\n",
      "Epoch 90/100 - Average Loss: 1.2682\n",
      "Epoch 100/100 - Average Loss: 1.1852\n",
      "\n",
      "CBOW Training complete!\n",
      "\n",
      "Learned Word Embeddings (from W1):\n",
      " learning: [ 1.00718011  0.3155199   0.51897213  0.86501559  0.69441692  0.11574587\n",
      "  0.71617234  0.59664605 -0.20703954  0.50952399]\n",
      " i: [-0.12579579  0.86966183  0.66529115  0.96391902  0.09858299  0.59549508\n",
      "  0.69182727  0.63117647  0.4317543   1.09385906]\n",
      " enjoy: [ 0.42768967  0.27413427  0.68875303  1.03082477 -0.3779149   0.74494544\n",
      "  0.45975044  0.49889164  0.7251779  -0.25186041]\n",
      " nlp: [0.48296039 0.8692693  0.88194447 0.531306   0.37096163 0.43633453\n",
      " 0.78119958 0.27461856 0.44045554 0.47731973]\n",
      " flying: [ 0.76020439  0.42120617 -0.13733072  0.18674891  0.4002503   0.70797344\n",
      "  0.6520983   0.20844408  0.02402607  1.04847912]\n",
      " deep: [0.91984664 1.16129764 0.80822202 0.32617409 1.16887369 0.54177501\n",
      " 0.05314125 0.58854909 0.36202945 0.2509632 ]\n",
      " like: [ 0.9502081   0.04146687  0.91920798  0.34354135 -0.00154168  0.27753919\n",
      " -0.48309971  0.10704054  0.98751374  0.44625544]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x in a numerically stable way.\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Define the Corpus\n",
    "# ---------------------------------------------------\n",
    "corpus = [\n",
    "    \"I like deep learning\",\n",
    "    \"I like NLP\",\n",
    "    \"I enjoy flying\"\n",
    "]\n",
    "\n",
    "print(\"Original Corpus:\")\n",
    "for sentence in corpus:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Preprocess the Corpus\n",
    "# Lowercase and tokenize each sentence.\n",
    "# ---------------------------------------------------\n",
    "sentences = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Build the Vocabulary and Mappings\n",
    "# ---------------------------------------------------\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)  # Convert set to list to have a fixed order\n",
    "\n",
    "# Create word-to-index and index-to-word mappings.\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(\"\\nVocabulary (word to index mapping):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {idx}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Generate Training Data for CBOW\n",
    "# In CBOW, given the context words, we try to predict the center word.\n",
    "# For each word in a sentence, the context is defined as the words\n",
    "# within a window size (excluding the center word itself).\n",
    "# ---------------------------------------------------\n",
    "window_size = 1\n",
    "training_pairs = []  # Each element is a tuple: (context_indices, center_index)\n",
    "\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        center_index = word2idx[word]\n",
    "        context_indices = []\n",
    "        # Collect words before the center word\n",
    "        for i in range(max(0, idx - window_size), idx):\n",
    "            context_indices.append(word2idx[sentence[i]])\n",
    "        # Collect words after the center word\n",
    "        for i in range(idx + 1, min(len(sentence), idx + window_size + 1)):\n",
    "            context_indices.append(word2idx[sentence[i]])\n",
    "        # Only add pairs where there is at least one context word\n",
    "        if context_indices:\n",
    "            training_pairs.append((context_indices, center_index))\n",
    "\n",
    "print(\"\\nTraining Pairs (context word indices, center word index):\")\n",
    "for context_idxs, center_idx in training_pairs:\n",
    "    context_words = [idx2word[idx] for idx in context_idxs]\n",
    "    center_word = idx2word[center_idx]\n",
    "    print(f\" Context: {context_words}, Center: {center_word} ({center_idx})\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 5: Initialize Hyperparameters and Weights\n",
    "# ---------------------------------------------------\n",
    "embedding_dim = 10        # Dimension of the embedding vector\n",
    "learning_rate = 0.01      # Learning rate for gradient descent\n",
    "epochs = 100              # Number of epochs to train\n",
    "vocab_size = len(vocab)   # Number of unique words in the vocabulary\n",
    "\n",
    "# Weight matrices:\n",
    "# W1: shape (vocab_size, embedding_dim) maps a one-hot vector to an embedding.\n",
    "# W2: shape (embedding_dim, vocab_size) maps the hidden representation to scores over vocabulary.\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 6: Training the CBOW Model\n",
    "# ---------------------------------------------------\n",
    "print(\"\\nStarting CBOW training...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0  # Accumulate loss over the epoch\n",
    "\n",
    "    # Process each training pair\n",
    "    for context_indices, center_idx in training_pairs:\n",
    "        # ---------- Forward Pass ----------\n",
    "        # 1. Look up embeddings for each context word from W1\n",
    "        context_embeddings = np.array([W1[idx] for idx in context_indices])\n",
    "        \n",
    "        # 2. Compute the hidden layer representation by averaging the context embeddings\n",
    "        h = np.mean(context_embeddings, axis=0)  # Shape: (embedding_dim,)\n",
    "        \n",
    "        # 3. Compute the scores over the vocabulary using W2\n",
    "        scores = np.dot(h, W2)  # Shape: (vocab_size,)\n",
    "        \n",
    "        # 4. Apply softmax to obtain predicted probabilities\n",
    "        y_pred = softmax(scores)\n",
    "        \n",
    "        # 5. Compute the loss (negative log likelihood for the true center word)\n",
    "        loss = -np.log(y_pred[center_idx] + 1e-7)  # Adding epsilon to avoid log(0)\n",
    "        loss_epoch += loss\n",
    "\n",
    "        # ---------- Backward Pass ----------\n",
    "        # 1. Compute error: the derivative of the loss with respect to the scores\n",
    "        y_true = np.zeros(vocab_size)\n",
    "        y_true[center_idx] = 1\n",
    "        error = y_pred - y_true  # Shape: (vocab_size,)\n",
    "        \n",
    "        # 2. Compute gradient for W2 as the outer product of h and the error\n",
    "        grad_W2 = np.outer(h, error)  # Shape: (embedding_dim, vocab_size)\n",
    "        \n",
    "        # 3. Compute gradient with respect to the hidden representation h\n",
    "        grad_h = np.dot(W2, error)  # Shape: (embedding_dim,)\n",
    "        \n",
    "        # 4. Since h is the average of the context embeddings,\n",
    "        #    distribute the gradient equally among them.\n",
    "        grad_context = grad_h / len(context_indices)\n",
    "        \n",
    "        # ---------- Update Weights ----------\n",
    "        # Update W1 for each context word in the training pair.\n",
    "        for idx in context_indices:\n",
    "            W1[idx] -= learning_rate * grad_context\n",
    "        \n",
    "        # Update W2\n",
    "        W2 -= learning_rate * grad_W2\n",
    "\n",
    "    # Print the average loss every 10 epochs for monitoring.\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = loss_epoch / len(training_pairs)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nCBOW Training complete!\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 7: Display the Learned Embeddings\n",
    "# ---------------------------------------------------\n",
    "print(\"\\nLearned Word Embeddings (from W1):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {W1[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram\n",
    "与根据周围上下文预测中心词的 CBOW 不同，skip-gram 则相反，它根据中心词预测周围的上下文词。例如，在句子“The cat sat on the mat”中，如果选择“sat”作为中心词，模型将尝试预测其上下文词 [“The”、“cat”、“on”、“the”]。\n",
    "\n",
    "![skip-gram](image-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      " - I like deep learning\n",
      " - I like NLP\n",
      " - I enjoy flying\n",
      "\n",
      "Tokenized Sentences:\n",
      " - ['i', 'like', 'deep', 'learning']\n",
      " - ['i', 'like', 'nlp']\n",
      " - ['i', 'enjoy', 'flying']\n",
      "\n",
      "Vocabulary (word to index mapping):\n",
      " learning: 0\n",
      " i: 1\n",
      " enjoy: 2\n",
      " nlp: 3\n",
      " flying: 4\n",
      " deep: 5\n",
      " like: 6\n",
      "\n",
      "Training Pairs (center word index, context word index):\n",
      " Center: i (1), Context: like (6)\n",
      " Center: like (6), Context: i (1)\n",
      " Center: like (6), Context: deep (5)\n",
      " Center: deep (5), Context: like (6)\n",
      " Center: deep (5), Context: learning (0)\n",
      " Center: learning (0), Context: deep (5)\n",
      " Center: i (1), Context: like (6)\n",
      " Center: like (6), Context: i (1)\n",
      " Center: like (6), Context: nlp (3)\n",
      " Center: nlp (3), Context: like (6)\n",
      " Center: i (1), Context: enjoy (2)\n",
      " Center: enjoy (2), Context: i (1)\n",
      " Center: enjoy (2), Context: flying (4)\n",
      " Center: flying (4), Context: enjoy (2)\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 10/100 - Average Loss: 1.7903\n",
      "Epoch 20/100 - Average Loss: 1.6539\n",
      "Epoch 30/100 - Average Loss: 1.5358\n",
      "Epoch 40/100 - Average Loss: 1.4188\n",
      "Epoch 50/100 - Average Loss: 1.3046\n",
      "Epoch 60/100 - Average Loss: 1.1993\n",
      "Epoch 70/100 - Average Loss: 1.1070\n",
      "Epoch 80/100 - Average Loss: 1.0290\n",
      "Epoch 90/100 - Average Loss: 0.9644\n",
      "Epoch 100/100 - Average Loss: 0.9117\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Learned Word Embeddings (from W1):\n",
      " learning: [-0.1006557   0.30009483  1.19590827  0.52171741 -0.1218098   0.82664764\n",
      "  0.73500598  0.89735935  0.76923992  0.50128099]\n",
      " i: [0.16153732 0.19169074 0.01134909 0.14571659 1.45750725 0.78297085\n",
      " 0.63308937 0.9961295  0.04032153 0.507321  ]\n",
      " enjoy: [ 1.01545029  0.70824457  0.68969117  1.34846364  0.47893864  0.01398807\n",
      "  0.17370369  0.05170594 -0.11561001  1.03247868]\n",
      " nlp: [-0.08629297  1.08424667  0.75780374  0.13931612  1.1016819   1.06490712\n",
      " -0.02274734  0.94226904  0.7910909   0.81424925]\n",
      " flying: [ 0.30651398 -0.39352399  0.62079155  0.5481522   0.85711093  0.71676915\n",
      "  1.42206289  0.92774372  0.80632766  0.43011506]\n",
      " deep: [ 0.80620003  0.90333533  0.37889582 -0.39248734  0.65825811  0.26336657\n",
      " -0.27837315  0.48462587  0.66492916  0.05313301]\n",
      " like: [ 1.04268798  0.53039035  1.02503911  1.01292128  0.07933664  0.95055247\n",
      "  0.46006448 -0.12682131  1.26019695  0.20153307]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x.\n",
    "    We subtract the maximum value for numerical stability.\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Define the Corpus\n",
    "# -------------------------\n",
    "corpus = [\n",
    "    \"I like deep learning\",\n",
    "    \"I like NLP\",\n",
    "    \"I enjoy flying\"\n",
    "]\n",
    "\n",
    "print(\"Original Corpus:\")\n",
    "for sentence in corpus:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# ------------------------------------\n",
    "# Step 2: Preprocess the Corpus\n",
    "# Lowercase and tokenize each sentence.\n",
    "# ------------------------------------\n",
    "sentences = [sentence.lower().split() for sentence in corpus]\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 3: Build the Vocabulary and Mappings\n",
    "# -----------------------------------------\n",
    "vocab = set()  # use a set to avoid duplicates\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)  # convert to list to fix ordering\n",
    "\n",
    "# Create dictionaries to map words to indices and vice-versa.\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(\"\\nVocabulary (word to index mapping):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {idx}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Step 4: Generate Training Data (Skip-gram Pairs)\n",
    "# -------------------------------------------------------\n",
    "# For each word in a sentence, use a window of size 1 to collect context words.\n",
    "window_size = 1\n",
    "training_pairs = []  # will store tuples of (center_word_idx, context_word_idx)\n",
    "\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        center_word_idx = word2idx[word]\n",
    "        # Determine the indices for the context window\n",
    "        context_indices = list(range(max(0, idx - window_size), idx)) + \\\n",
    "                          list(range(idx + 1, min(len(sentence), idx + window_size + 1)))\n",
    "        for context_idx in context_indices:\n",
    "            context_word_idx = word2idx[sentence[context_idx]]\n",
    "            training_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "print(\"\\nTraining Pairs (center word index, context word index):\")\n",
    "for center, context in training_pairs:\n",
    "    print(f\" Center: {idx2word[center]} ({center}), Context: {idx2word[context]} ({context})\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 5: Initialize Hyperparameters and Weight Matrices\n",
    "# ------------------------------------------------------\n",
    "embedding_dim = 10       # size of the embedding vector\n",
    "learning_rate = 0.01     # learning rate for SGD updates\n",
    "epochs = 100             # number of epochs for training\n",
    "vocab_size = len(vocab)  # number of unique words\n",
    "\n",
    "# Weight matrices:\n",
    "# W1: shape (vocab_size, embedding_dim) - maps one-hot input to embeddings\n",
    "# W2: shape (embedding_dim, vocab_size) - maps embeddings to scores over vocabulary\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "# --------------------------------\n",
    "# Step 6: Training the Model\n",
    "# --------------------------------\n",
    "print(\"\\nStarting training...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0  # accumulate loss over the epoch\n",
    "    \n",
    "    # Iterate through each training pair\n",
    "    for center_idx, context_idx in training_pairs:\n",
    "        # ---------- Forward Pass ----------\n",
    "        # 1. Look up the embedding for the center word (from W1)\n",
    "        center_embedding = W1[center_idx]  # shape: (embedding_dim,)\n",
    "        \n",
    "        # 2. Compute scores for all words by multiplying the embedding with W2\n",
    "        scores = np.dot(center_embedding, W2)  # shape: (vocab_size,)\n",
    "        \n",
    "        # 3. Apply softmax to get probabilities over the vocabulary\n",
    "        y_pred = softmax(scores)  # shape: (vocab_size,)\n",
    "        \n",
    "        # 4. Compute the loss (negative log likelihood for the true context word)\n",
    "        loss = -np.log(y_pred[context_idx] + 1e-7)  # add a small number to prevent log(0)\n",
    "        loss_epoch += loss\n",
    "\n",
    "        # ---------- Backward Pass ----------\n",
    "        # Create a one-hot encoded vector for the true context word\n",
    "        y_true = np.zeros(vocab_size)\n",
    "        y_true[context_idx] = 1\n",
    "        \n",
    "        # Compute the error: derivative of loss with respect to the scores\n",
    "        error = y_pred - y_true  # shape: (vocab_size,)\n",
    "        \n",
    "        # Compute gradients for W2 and the center embedding:\n",
    "        # Gradient for W2 is the outer product of the center embedding and the error\n",
    "        grad_W2 = np.outer(center_embedding, error)  # shape: (embedding_dim, vocab_size)\n",
    "        \n",
    "        # Gradient for the center embedding (W1 row) is the dot product of W2 and the error\n",
    "        grad_center = np.dot(W2, error)  # shape: (embedding_dim,)\n",
    "        \n",
    "        # ---------- Update Weights ----------\n",
    "        # Update the embedding for the center word in W1\n",
    "        W1[center_idx] -= learning_rate * grad_center\n",
    "        \n",
    "        # Update W2 with the computed gradient\n",
    "        W2 -= learning_rate * grad_W2\n",
    "\n",
    "    # Print the average loss every 10 epochs for monitoring\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = loss_epoch / len(training_pairs)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# --------------------------------------\n",
    "# Step 7: Display the Learned Embeddings\n",
    "# --------------------------------------\n",
    "print(\"\\nLearned Word Embeddings (from W1):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {W1[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "Word2Vec 通过预测上下文来捕捉局部单词关系，而 GloVe 则采取了更广泛的方法，通过分解全局单词共现矩阵来导出低维嵌入。通过这样做，它可以发现从狭隘的逐句视角来看可能看不见的模式和关系。\n",
    "本质上，GloVe 旨在以一种能够捕捉全局和局部背景的方式对单词之间的关系进行建模，从而生成能够反映语义相似性和有意义的单词关系（如类比）的词向量（例如，“国王”-“男人”+“女人”≈“女王”）。\n",
    "\n",
    "![GloVe示例](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:\n",
      " - I like deep learning\n",
      " - I like NLP\n",
      " - I enjoy flying\n",
      "\n",
      "Tokenized Sentences:\n",
      " - ['i', 'like', 'deep', 'learning']\n",
      " - ['i', 'like', 'nlp']\n",
      " - ['i', 'enjoy', 'flying']\n",
      "\n",
      "Vocabulary (word to index mapping):\n",
      " learning: 0\n",
      " i: 1\n",
      " enjoy: 2\n",
      " nlp: 3\n",
      " flying: 4\n",
      " deep: 5\n",
      " like: 6\n",
      "\n",
      "Co-occurrence Matrix (X):\n",
      "[[0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 2. 0. 1. 0. 1. 0.]]\n",
      "Epoch 10/100, Total Cost: 2.4720\n",
      "Epoch 20/100, Total Cost: 1.0695\n",
      "Epoch 30/100, Total Cost: 0.5459\n",
      "Epoch 40/100, Total Cost: 0.3112\n",
      "Epoch 50/100, Total Cost: 0.1927\n",
      "Epoch 60/100, Total Cost: 0.1271\n",
      "Epoch 70/100, Total Cost: 0.0882\n",
      "Epoch 80/100, Total Cost: 0.0636\n",
      "Epoch 90/100, Total Cost: 0.0472\n",
      "Epoch 100/100, Total Cost: 0.0359\n",
      "\n",
      "Learned GloVe Embeddings:\n",
      " learning: [ 0.5583196   0.77602033  1.15798507  1.52328133  1.46379766 -0.03221975\n",
      "  0.76586303  0.65021835  1.23179541  0.88127723]\n",
      " i: [ 0.88461254  0.3229078  -0.59739749  0.26060134  1.39084396  0.92086083\n",
      "  0.44186215  0.56712187  1.20328921  0.94818174]\n",
      " enjoy: [ 0.99574884  1.32528307  1.47531431  1.04992844  0.07833501  0.31521111\n",
      "  0.28730792  1.0524848   0.2347046  -0.34827608]\n",
      " nlp: [0.62344344 0.38411663 1.09937796 0.93354282 1.01878098 0.49544195\n",
      " 0.63130683 0.46160609 1.23100129 0.60859278]\n",
      " flying: [-0.1022411   0.24705036  0.84021702  0.31010817  0.61160166  1.38943156\n",
      "  1.33211608  0.70219365  0.48111987  1.29938664]\n",
      " deep: [0.91061228 0.97278126 0.04699649 0.37339865 0.16859089 0.69341562\n",
      " 0.07478501 0.64114991 0.23072767 0.24213257]\n",
      " like: [0.57366931 0.22067052 1.00295081 0.39387644 0.55816843 0.54118635\n",
      " 0.48925779 0.57876234 0.03515956 0.94288348]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weighting_func(x, x_max=100, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Compute the weighting function for a co-occurrence count.\n",
    "    If x < x_max, return (x / x_max)^alpha; otherwise return 1.\n",
    "    \"\"\"\n",
    "    return (x / x_max) ** alpha if x < x_max else 1\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Define the Corpus\n",
    "# ---------------------------------------------------\n",
    "corpus = [\n",
    "    \"I like deep learning\",\n",
    "    \"I like NLP\",\n",
    "    \"I enjoy flying\"\n",
    "]\n",
    "\n",
    "print(\"Original Corpus:\")\n",
    "for sentence in corpus:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Preprocess the Corpus\n",
    "# Lowercase and tokenize each sentence.\n",
    "# ---------------------------------------------------\n",
    "sentences = [sentence.lower().split() for sentence in corpus]\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(\" -\", sentence)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Build the Vocabulary and Mappings\n",
    "# ---------------------------------------------------\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(\"\\nVocabulary (word to index mapping):\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {idx}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Build the Co-occurrence Matrix\n",
    "# We'll use a window size of 1 for simplicity.\n",
    "# ---------------------------------------------------\n",
    "vocab_size = len(vocab)\n",
    "X = np.zeros((vocab_size, vocab_size))\n",
    "window_size = 1\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_length = len(sentence)\n",
    "    for i, word in enumerate(sentence):\n",
    "        word_idx = word2idx[word]\n",
    "        # Define the window boundaries\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(sentence_length, i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i == j:\n",
    "                continue  # Skip the word itself\n",
    "            context_word = sentence[j]\n",
    "            context_idx = word2idx[context_word]\n",
    "            X[word_idx, context_idx] += 1\n",
    "\n",
    "print(\"\\nCo-occurrence Matrix (X):\")\n",
    "print(X)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 5: Initialize GloVe Parameters\n",
    "# ---------------------------------------------------\n",
    "embedding_dim = 10      # Dimension of the embeddings\n",
    "learning_rate = 0.05\n",
    "epochs = 100\n",
    "\n",
    "# Initialize word and context embeddings randomly\n",
    "W = np.random.rand(vocab_size, embedding_dim)\n",
    "W_context = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Initialize bias terms for words and context words\n",
    "b = np.random.rand(vocab_size)\n",
    "b_context = np.random.rand(vocab_size)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 6: Train the GloVe Model\n",
    "# ---------------------------------------------------\n",
    "# We minimize the cost: f(X_ij) * (w_i^T w_j~ + b_i + b_j~ - log(X_ij))^2\n",
    "for epoch in range(epochs):\n",
    "    total_cost = 0\n",
    "    # Iterate over all nonzero co-occurrence entries\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if X[i, j] > 0:\n",
    "                # Compute weighting for this co-occurrence\n",
    "                weight = weighting_func(X[i, j])\n",
    "                # Calculate the difference between prediction and log count\n",
    "                diff = np.dot(W[i], W_context[j]) + b[i] + b_context[j] - np.log(X[i, j])\n",
    "                cost = weight * (diff ** 2)\n",
    "                total_cost += cost\n",
    "                # Compute gradient (the factor 2 comes from the derivative of the square)\n",
    "                grad = 2 * weight * diff\n",
    "                # Update the parameters using gradient descent\n",
    "                W[i] -= learning_rate * grad * W_context[j]\n",
    "                W_context[j] -= learning_rate * grad * W[i]\n",
    "                b[i] -= learning_rate * grad\n",
    "                b_context[j] -= learning_rate * grad\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Total Cost: {total_cost:.4f}\")\n",
    "\n",
    "# Combine word and context embeddings as the final representation\n",
    "final_embeddings = W + W_context\n",
    "\n",
    "print(\"\\nLearned GloVe Embeddings:\")\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\" {word}: {final_embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 和 GloVe 等方法也有局限性——它们通常很难捕捉句子中的完整单词序列，尤其是当上下文跨越许多标记或整个段落时。这就是神经序列模型发挥作用的地方，它们建立在这些嵌入的基础之上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用神经元构建上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列模型\n",
    "### CNN - 捕获局部\n",
    "捕获局部特征，主要用于分类\n",
    "\n",
    "![CNN可视化](image-7.png)\n",
    "\n",
    "### RNN - 引入记忆\n",
    "循环神经网络 (RNN) 就像讲故事的人，在展开故事的同时回忆过去。RNN 采用了循环机制，通过隐藏状态构建时间记忆。RNN 按顺序处理输入（一次一个单词），同时保持此隐藏状态，即从先前的时间步骤中积累信息的向量。此隐藏状态在每个时间步骤使用当前输入和先前的隐藏状态进行更新，从而有效地传递序列上下文。\n",
    "只要数据的序列或顺序很重要，RNN 就会表现出色。它们已广泛应用于情绪分析、机器翻译和文本生成等任务，但也扩展到非文本数据，如音频信号（语音识别）或金融时间序列（股票预测）。\n",
    "Disadvantages：当序列变得很长时，RNN 可能会面临梯度消失或爆炸等挑战，这使得学习远距离关系变得困难。这个问题促使人们创建了更先进的架构，如 LSTM、GRU，以及最终的 Transformers，它们使用自注意力并行处理 token。\n",
    "\n",
    "![RNN可视化](image-6.png)\n",
    "\n",
    "### LSTM - 长短期记忆网络\n",
    "RNN 有一个明显的局限性：当它们处理较长的序列时，它们很难从序列的开头保留信息。长短期记忆 (LSTM) 网络是一种专门用于克服这一挑战的 RNN。\n",
    "LSTM 拥有复杂的记忆系统，能够跟踪长序列中的重要信息，而不会被无关细节所困扰。这种能力对于上下文和长期依赖关系很重要的任务至关重要，例如理解段落的含义或根据历史数据预测未来股票价格。\n",
    "每个 LSTM 的核心都是一个门控架构，由三个关键组件组成：输入门、遗忘门和输出门，它们管理信息流入和流出 LSTM 的内部记忆或单元状态。这些门会评估新信息和现有信息，从而使 LSTM 能够选择性地更新其记忆。\n",
    "例如，输入门决定是否应将新信息添加到记忆中，遗忘门决定是否应丢弃旧信息，输出门选择哪些信息应该影响网络的输出。这种选择性过程确保 LSTM 只保留最相关的细节，同时过滤掉其余信息。通过动态调整这些门，LSTM 可以平衡记住重要信息和丢弃不必要的信息。这种选择性记忆过程使 LSTM 能够处理比传统 RNN 更长的序列，使其在各种应用中非常强大。\n",
    "\n",
    "![LSTM可视化](image-8.png)\n",
    "\n",
    "### CNN、RNN和LSTM比对\n",
    "| Model | Strengths  优势          | Limitations  限制   | Typical Use Cases  典型用例  |\n",
    "|-------|------------------------|-------------------|--------------------------|\n",
    "| CNN   | 擅长捕捉局部模式（例如，检测 n-gram） | 固定窗口大小限制上下文捕获        | 文本分类                     |\n",
    "|       | 可并行化，速度非常快                 | 无法有效建模长期依赖关系          | NLP 中的特征提取               |\n",
    "|       | 局部特征提取的计算成本低              | 不适用于顺序数据处理             | 图像识别                     |\n",
    "| RNN   | 按顺序处理输入，保留时间上下文        | 长序列容易出现梯度消失或爆炸      |   语言建模                   |\n",
    "|       | 处理可变长度序列                     | 捕获长距离依赖关系的能力有限      | 语音识别                     |\n",
    "|       | 序列建模的简单架构                   | 顺序性阻碍了并行处理              | 基本序列预测                   |\n",
    "| LSTM  | 门控架构克服了梯度消失问题            | 更复杂，计算量更大                | 机器翻译                     |\n",
    "|       | 比简单的 RNN 更善于捕捉长程依赖关系   | 仍然受到顺序处理的限制，影响并行性  | 机器翻译                     |\n",
    "|       | 随着时间的推移有选择地保留重要信息     | 由于复杂性增加，训练时间更长       | 时间序列预测                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用编码器-解码器学习短语表征\n",
    "我们已经了解了 RNN 和 LSTM 如何通过一步一步处理输入来处理序列，同时通过隐藏状态保持上下文。这种方法非常适合预测下一个单词或对句子的情感进行分类等任务，但许多现实世界的问题需要将整个输入序列（如英文段落）映射到完整的输出序列（如法语翻译）。仅仅预测下一个标记或为整个序列分配一个标签是不够的。\n",
    "例如，将“我喜欢猫”翻译成“我爱猫”，需要模型阅读并完全理解完整的英语句子，然后才能从头到尾生成法语句子。经典的 RNN 将所有信息压缩到其最终隐藏状态中。但是，如果输入的句子更复杂——比如“昨天，在大型音乐厅演出的杰出音乐家被邀请明年夏天来演出”——单个隐藏状态最终可能会丢失或混淆关键细节，尤其是当“昨天”等重要元素出现在开头，而“明年夏天”等关键上下文出现在结尾时。\n",
    "这一挑战被称为瓶颈问题：一旦 RNN 处理了整个序列，它必须将所有信息压缩成一个压缩向量，然后才能生成输出。如果该向量无法捕捉到基本细微差别（例如事件的时间或不同主题的具体角色），则生成的翻译或摘要可能会变得杂乱无章。简而言之，序列到序列模型中的瓶颈就像将整部小说塞进一条推文中！我们不能这样做！\n",
    "将 RNN 分为两个不同的部分——编码器和解码器——以便每个部分都能专注于其角色。\n",
    "编码器：仔细处理整个源句子、吸收细节并构建内部摘要的“听众”。\n",
    "\n",
    "![编码器架构](image-9.png)\n",
    "\n",
    "解码器：使用该摘要来生成另一种语言的新序列，甚至以摘要或标题等其他形式生成的“讲故事的人”。\n",
    "\n",
    "![解码器架构](image-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成建模\n",
    "### VAE（变分自动编码器）\n",
    "![VAE架构](image-11.png)\n",
    "### GAN（生成对抗网络）\n",
    "GAN 由两个神经网络组成——生成器和鉴别器，它们在零和游戏中同时训练。生成器的目标是生成与真实数据无法区分的输出（例如图像），而鉴别器的目标是正确分类给定样本是真实的还是生成的。这种动态相互作用创造了一个强大的反馈循环，两个网络通过竞争不断改进。\n",
    "![GAN架构](image-12.png)\n",
    "### 总结\n",
    "|方面 |VAE |GAN\n",
    "|----|---------------------|-----------------------\n",
    "|方法      |概率建模：使用均值和方差，通过 KL 散度正则化器将输入映射到潜在分布。|对抗性训练：在极小最大游戏中使用生成器和鉴别器。\n",
    "|损失函数  |重建损失和 KL 散度损失的组合。                                   |生成器（用于欺骗鉴别器）和鉴别器（用于区分真假）的对抗性损失函数。\n",
    "|训练稳定性|通常更稳定且更容易训练，但输出可能不太敏锐。                       |训练可能不稳定（例如模式崩溃），但如果成功，则可以产生高度逼真的输出。\n",
    "|输出质量  |通常会产生多样化的样本，但有时会以牺牲图像清晰度为代价。            |尽管多样性可能是一个挑战，但能够产生逼真、细致的图像。\n",
    "|潜在空间  |提供可用于插值的平滑且可解释的潜在空间。                           |没有强制执行明确的潜在空间；重点是生成真实的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "模型在生成输出时如何调用和利用输入序列中最相关的部分，尤其是在处理长数据或复杂数据时？解决方案是注意力机制。\n",
    "注意力机制的引入是为了解决早期编码器-解码器模型中的一个紧迫挑战：将输入序列的相关信息压缩为单个上下文向量通常会导致丢失关键细节，尤其是对于长数据或复杂数据。\n",
    "注意力机制不再依赖于一个固定的摘要，而是允许模型在输出生成的每个步骤中动态地关注输入的不同部分。\n",
    "\n",
    "![注意力如何只关注重要信息](image-13.png)\n",
    "\n",
    "想象一下，您正在观看一场戏剧演出。在传统的编码器-解码器模型中，上下文向量就像一盏昏暗的聚光灯，试图照亮整个舞台——许多重要的演员（或细节）仍然在阴影中。有了注意力，模型就会获得多个可调节的聚光灯，可以根据当时的需求瞄准舞台的特定区域。对于它生成的每个单词，模型都会照亮输入中最相关的部分，确保不会丢失细微的差别，并且最终输出保持连贯且语境丰富。\n",
    "这种机制的核心是一个简单而强大的想法，涉及三个组成部分：查询、键和值（通常缩写为 Q、K、V）。将查询视为您现在要问的问题 - 我需要什么信息？每个输入元素都带有一个键（用作描述性标签）和一个值（即该元素中包含的实际信息）。该模型通过计算查询和每个键之间的点积来计算对齐分数，缩放这些分数，然后应用 softmax 函数将它们转换为概率。此过程确定输入的哪些部分最相关 - 就像评估一个组织良好的笔记本中各种笔记的重要性一样。\n",
    "1、比较相似度\n",
    "$$\n",
    "    score=Q⋅K\n",
    "$$\n",
    "2、转换为概率（scalling和softmax）\n",
    "$$\n",
    "    \\alpha_{i}=\\operatorname{softmax}\\left(\\frac{Q \\cdot K_{i}}{\\sqrt{d_{k}}}\\right)=\\frac{e^{\\frac{Q \\cdot K_{i}}{\\sqrt{d_{k}}}}}{\\sum_{j} e^{\\frac{Q \\cdot K_{j}}{\\sqrt{d_{k}}}}}\n",
    "$$\n",
    "3、创建自定义摘要（加权和）\n",
    "$$\n",
    "    \\text { Output }=\\sum_{i} \\alpha_{i} V_{i}\n",
    "$$\n",
    "\n",
    "![注意力机制](image-14.png)\n",
    "\n",
    "注意力机制使模型能够自适应地记住和利用最相关的细节，从而产生更准确、更流畅的输出。\n",
    "### Transformer架构\n",
    "Transformer 利用注意力机制让句子中的每个单词直接与其他每个单词交互，从而消除了 RNN 和 LSTM 固有的顺序瓶颈。\n",
    "#### 自注意力机制\n",
    "Transformer 架构的核心是自注意力机制，它计算一组权重，描述输入序列中每个单词与其他每个单词的相关性。想象一下，当你阅读一个复杂的段落时，在任何给定时刻，你都会本能地关注那些对理解当前上下文最重要的单词。同样，在 Transformer 中，自注意力层计算所有标记对之间的注意力分数，使模型能够在编码给定单词时动态权衡每个单词的影响。此过程会产生上下文化的表示，捕捉整个序列中微妙的依赖关系和关系。\n",
    "\n",
    "![原始transformer架构可视化](image-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Self-Attention Computation ===\n",
      "\n",
      "Step 1: Dot Product (Similarity Calculation)\n",
      "Original query: [0.1 0.2 0.3 0.4]\n",
      "Reshaped query (for multiplication): [[0.1 0.2 0.3 0.4]]\n",
      "Transposed keys:\n",
      " [[0.5 0.9 1.3]\n",
      " [0.6 1.  1.4]\n",
      " [0.7 1.1 1.5]\n",
      " [0.8 1.2 1.6]]\n",
      "Resulting dot product: [[0.7 1.1 1.5]]\n",
      "\n",
      "Step 2: Softmax Normalization\n",
      "Exponentiated dot product: [[2.01375271 3.00416602 4.48168907]]\n",
      "Sum of exponentiated scores: [[9.4996078]]\n",
      "Attention weights after softmax normalization: [[0.21198272 0.31624106 0.47177622]]\n",
      "\n",
      "Step 3: Weighted Sum of Values (Creating the Output)\n",
      "Output vector (weighted sum of values): [[1.0039174 1.1039174 1.2039174 1.3039174]]\n",
      "\n",
      "=== Final Results ===\n",
      "Final Output of Self-Attention: [[1.0039174 1.1039174 1.2039174 1.3039174]]\n",
      "Final Attention Weights: [[0.21198272 0.31624106 0.47177622]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample input: a sentence represented as word embeddings\n",
    "sentence = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],   # \"it\"\n",
    "    [0.5, 0.6, 0.7, 0.8],   # \"refers\"\n",
    "    [0.9, 1.0, 1.1, 1.2],   # \"to\"\n",
    "    [1.3, 1.4, 1.5, 1.6],   # \"robot\"\n",
    "    [1.7, 1.8, 1.9, 2.0]    # \".\"\n",
    "])\n",
    "\n",
    "def self_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    Demonstrates self-attention for one query with detailed outputs at each step.\n",
    "    \n",
    "    Parameters:\n",
    "        query: A single query vector.\n",
    "        keys: Multiple key vectors.\n",
    "        values: Multiple value vectors.\n",
    "    \n",
    "    Returns:\n",
    "        output: The weighted sum of the values (the attention output).\n",
    "        attention_weights: The computed attention weights.\n",
    "    \"\"\"\n",
    "    print(\"=== Self-Attention Computation ===\")\n",
    "    \n",
    "    # Step 1: Dot Product between Query and Keys\n",
    "    print(\"\\nStep 1: Dot Product (Similarity Calculation)\")\n",
    "    print(\"Original query:\", query)\n",
    "    query = query[np.newaxis, :]  # Reshape query for matrix multiplication\n",
    "    print(\"Reshaped query (for multiplication):\", query)\n",
    "    \n",
    "    keys_transposed = keys.T      # Transpose keys for proper alignment\n",
    "    print(\"Transposed keys:\\n\", keys_transposed)\n",
    "    \n",
    "    dot_product = np.dot(query, keys_transposed)\n",
    "    print(\"Resulting dot product:\", dot_product)\n",
    "    \n",
    "    # Step 2: Apply Softmax to Convert Dot Products to Probabilities\n",
    "    print(\"\\nStep 2: Softmax Normalization\")\n",
    "    exp_dot_product = np.exp(dot_product)\n",
    "    print(\"Exponentiated dot product:\", exp_dot_product)\n",
    "    \n",
    "    sum_exp = exp_dot_product.sum(axis=1, keepdims=True)\n",
    "    print(\"Sum of exponentiated scores:\", sum_exp)\n",
    "    \n",
    "    attention_weights = exp_dot_product / sum_exp\n",
    "    print(\"Attention weights after softmax normalization:\", attention_weights)\n",
    "    \n",
    "    # Step 3: Weighted Sum of Values to Get the Output\n",
    "    print(\"\\nStep 3: Weighted Sum of Values (Creating the Output)\")\n",
    "    output = np.dot(attention_weights, values)\n",
    "    print(\"Output vector (weighted sum of values):\", output)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# \"it\" is our query; the words \"refers\", \"to\", \"robot\" act as keys and values.\n",
    "query = sentence[0]          # \"it\"\n",
    "keys = sentence[1:-1]        # \"refers\", \"to\", \"robot\"\n",
    "values = sentence[1:-1]      # In this simple example, keys and values are the same\n",
    "\n",
    "# Perform self-attention computation\n",
    "output, attn_weights = self_attention(query, keys, values)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"Final Output of Self-Attention:\", output)\n",
    "print(\"Final Attention Weights:\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 位置编码\n",
    "由于自注意力将所有 token 视为没有固有顺序的集合，因此位置编码将有关每个 token 位置的信息注入模型。可以将其想象为在书中添加章节和页码：虽然内容保持不变，但附加信息有助于读者理解叙述的结构和流程。通过将这些位置信号与注意力衍生的上下文表示相结合，Transformer 可以保持连贯语言理解所必需的顺序和结构。\n",
    "学习绝对位置嵌入\n",
    "用不同的地址标记每个位置。\n",
    "相对位置编码\n",
    "用标记之间的距离。\n",
    "旋转位置嵌入\n",
    "它们将基于旋转的变换应用于标记嵌入，以便位置信息与标记表示不断交织在一起。这可以提高非常长序列的性能，优雅地扩展到训练中看到的位置之外。将每个标记想象成有一个小箭头（一个矢量），该箭头在序列中每向前一步都会旋转更多。当您移动到下一个标记时，您会进一步旋转箭头，连续且循环地嵌入位置。RoPE 将基于旋转的变换应用于标记嵌入，因此它们的角度与标记在序列中的位置相关。这将位置直接与每个标记的表示联系起来。\n",
    "#### 编码器-解码器结构\n",
    "![基本的编码器-解码器架构](image-16.png)\n",
    "\n",
    "#### RNN/LSTM和Transformer差异比对\n",
    "|方面 |RNN/LSTM |Transformer\n",
    "|-----|-------------------|-----------------\n",
    "|处理方法    |按顺序处理数据；并行性有限        |使用自注意力机制并行处理数据\n",
    "|长距离依赖  |梯度消失风险；上下文有限          |通过自注意力机制有效捕获长程依赖关系\n",
    "|训练效率    |由于顺序计算而速度较慢            |由于可并行架构，训练速度更快\n",
    "|架构       |依赖于循环；可以很深入，但更难训练  |采用层规范化、残差连接和多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于语言理解的双向Transformer\n",
    "BERT就是这种机制的典型示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估大语言模型\n",
    "### 内在评估指标\n",
    "#### 困惑度\n",
    "困惑度衡量概率模型（如语言模型）预测样本的准确程度。它告诉我们模型在看到测试数据时的惊讶程度。困惑度越低，表示模型的惊讶程度越低，这通常表明它能更好地预测句子中的下一个单词。\n",
    "#### BLEU\n",
    "它衡量生成的文本与一组参考文本（基本事实）的匹配程度。想象一下，你写了一篇文章，然后将它与一篇你知道很优秀的范文进行比较。你可能会寻找两篇文章中都出现的常用短语或模式。BLEU 的作用类似——它将生成的文本中的 n-gram（单词组）与参考文本中的 n-gram 进行比较。重叠的 n-gram 越多，得分就越高\n",
    "#### FID\n",
    "FID 通过测量生成特征的分布与学习特征空间中真实特征之间的距离来评估生成样本的质量（通常使用预训练网络（如图像的 Inception）提取）。\n",
    "### 外在评估指标\n",
    "问答\n",
    "文本分类\n",
    "命名实体识别\n",
    "语音转文本\n",
    "准确度\n",
    "偏差指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFT和RLHF\n",
    "\n",
    "Pre-training\n",
    "监督学习\n",
    "无监督学习\n",
    "自监督学习\n",
    "    因果策略 - GPT\n",
    "    掩蔽策略 - BERT\n",
    "    对比策略 - CLIP\n",
    "    基于语音的范式 - Whisper\n",
    "    基于扩散的范式 - DALLE\n",
    "预训练流程\n",
    "数据收集-预处理-模块初始化-自监督任务-损失计算-参数更新-大规模迭代\n",
    "\n",
    "Post-training、微调和调整\n",
    "什么是微调？\n",
    "本质上， 微调采用预训练模型（已经具有丰富的通用知识），并在较小的专业数据集上进一步训练它。\n",
    "\n",
    "![微调过程概述](image-17.png)\n",
    "\n",
    "迁移学习 - 微调的基础\n",
    "迁移学习涉及使用从解决一个问题（源）中获得的知识来改进相关问题（目标）的学习。它可以加快训练速度，减少对海量数据集的需求，并提高性能——当特定于任务的数据有限时尤其有益。\n",
    "\n",
    "![迁移学习概述](image-18.png)\n",
    "\n",
    "全面微调\n",
    "\n",
    "PEFT（参数高效微调）\n",
    "PEFT 背后的核心思想简单而强大：您无需更新每个参数，只需调整一小部分即可。这大大降低了计算成本、数据需求和过度拟合风险。\n",
    "基于特征的微调\n",
    "冻结预训练层-添加新层-仅训练新层\n",
    "选择性微调\n",
    "选择性微调进一步完善了这个想法。您无需冻结所有早期层，而是有选择地仅更新与您的任务高度相关的某些预训练层。\n",
    "冻结大多数层-有选择的更新关键层\n",
    "基于适配器的微调\n",
    "插入适配器-冻结预训练权重-仅训练适配器\n",
    "LoRA（Low-rank adaptation）\n",
    "以小而简化的（低秩）形式表示微调更新，而不是直接调整参数：\n",
    "冻结预训练权重-注入小型低秩矩阵-仅对低秩矩阵进行微调\n",
    "\n",
    "![参数微调总结](image-19.png)\n",
    "\n",
    "指令微调\n",
    "指令微调是一种特殊的微调形式，旨在使语言模型更好地理解和响应人类指令。我们不是简单地向模型提供来自书籍、文章或互联网的更一般的示例，而是创建一个特定的数据集，将自然语言指令与所需的输出配对。关键思想很简单：明确地教模型当人类给出指令时，良好的响应是什么样的\n",
    "创建指令数据集\n",
    "监督微调\n",
    "增强模型的多功能性\n",
    "\n",
    "RLHF（人类反馈进行强化学习）\n",
    "\n",
    "![RLHF概述](image-20.png)\n",
    "\n",
    "从预训练开始（Frozen LM：冻结了该模型参数，将其作为基准来衡量模型在微调过程中的变化程度）\n",
    "监督微调\n",
    "训练奖励模型\n",
    "使用PPO（proximal policy optimization）进行强化学习\n",
    "避免过度优化 - 使用Kullback–Leibler (KL) divergence加入惩罚\n",
    "\n",
    "微调常见陷阱\n",
    "过拟合\n",
    "数据不匹配\n",
    "计算资源\n",
    "\n",
    "模型部署优化\n",
    "知识蒸馏\n",
    "知识蒸馏的核心是训练一个较小、较简单的模型（学生模型）来模仿较大、高度准确的教师模型的预测。\n",
    "它的工作原理如下：首先，你要训练一个教师模型，通常是一个非常庞大、功能强大的网络，比如 GPT-4.5。一旦它经过精细调整并具有高度准确性，你就可以引入一个较小的学生模型——它可能具有更少的层、更少的参数，或者只是一个专为速度和效率而设计的更轻的结构。奇迹发生在下一步：训练学生模型。\n",
    "学生不是仅使用标准标记数据（硬标签）从头开始训练学生，而是通过密切观察老师的预测来学习。学生看到正确答案（硬标签）并研究老师的软概率——老师为每个可能的输出分配的详细概率分布。\n",
    "\n",
    "![知识蒸馏](image-21.png)\n",
    "\n",
    "为了确保有效的知识转移，训练通常结合两种损失函数：通常的任务特定损失（衡量学生预测正确答案的准确程度）和特殊的蒸馏损失 （衡量学生与老师的软预测的匹配程度）。这就像教一个人不仅要得到正确的答案，还要像他们的导师那样推理。\n",
    "量化\n",
    "量化涉及降低这些数字的精度。\n",
    "\n",
    "![量化示例](image-22.png)\n",
    "\n",
    "在训练后量化 - 首先以高精度全面训练模型，然后将模型的权重和激活转换为较低精度的格式。这个过程简单快捷，但准确性可能会略有下降。\n",
    "量化感知训练 - 训练过程中模拟量化过程，从一开始就教会模型适应较低精度的数字。由于模型从一开始就意识到量化，因此准确率往往更高。\n",
    "模型修剪\n",
    "神经网络中并非每个连接（权重）或神经元都对其最终性能有重大贡献。 模型修剪就是仔细修剪这些不太重要的部分，留下更精简、更快、更高效的网络。\n",
    "\n",
    "![修剪示例](image-23.png)\n",
    "\n",
    "权重修剪 - 识别出对网络准确性贡献不大的单个权重并将其设置为零。\n",
    "神经元修剪 - 移除整个神经元或过滤器。这类似于切断整个树枝而不是几根小枝，从而产生更简单、更适合硬件的结构，更易于运行且运行速度更快，尤其是在专用硬件上。\n",
    "\n",
    "\n",
    "扩展大语言模型\n",
    "MoE（minture of experts）模型\n",
    "专家：经过训练的子网络可以处理某些输入方面（例如数学、代码或日常语言）。\n",
    "路由器：一种门控机制，决定每个令牌或输入段使用哪个专家。\n",
    "\n",
    "![MoE架构](image-24.png)\n",
    "\n",
    "推理模型\n",
    "推理模型旨在分解需要多个步骤的任务，例如解决棘手的数学问题、调试一段代码或回答基于逻辑的问题。\n",
    "扩展上下文窗口\n",
    "位置外推和内插\n",
    "滑动窗口和分段\n",
    "内存高效的注意力机制\n",
    "硬件优化和flashattention\n",
    "\n",
    "视觉模型\n",
    "Vision Transformer（ViT）\n",
    "将图像转换成 Transformers 可以自然处理的东西，比如视觉句子。\n",
    "\n",
    "![视觉模型架构](image-25.png)\n",
    "\n",
    "工作原理\n",
    "图像补丁：eg：将28*28像素的图像切成7*7像素的块\n",
    "块的线性嵌入：Vision Transformers 首先将每个块展平 - 想象将每个图块展开成一长串像素值。这会为每个块创建一个长长的数字列表。然后，该模型使用一种称为线性投影的东西，这是一种简单的神经网络层，可将这些数字列表转换为称为块嵌入的特殊数字代码。\n",
    "位置嵌入：默认情况下，Transformer 是无序的 。但是图像要求有序，为了解决这个问题，Vision Transformer 添加了称为位置嵌入的特殊标记。\n",
    "Transformer编码器：这是 Vision Transformer 的真正强大之处，由配备两个关键组件的层组成：自注意力层和前馈层。自注意力层允许每个视觉词仔细查看图像中的所有其他视觉词。想象一下，每个图像块都向其他每个图像块低声询问：“你和我有关吗？我们属于同一个物体吗？”例如，带有猫耳朵的图像块将密切关注猫的脸、尾巴和爪子的图像块，意识到它们属于一起形成一只完整的猫。前馈层进一步细化理解，确保每个图像块嵌入都包含更丰富的视觉含义。\n",
    "分类头：\n",
    "蒙版图像建模\n",
    "MAE编码器：此部分通常是 Vision Transformer。它只查看可见的块，就像只接收拼图的几个碎片一样。它的工作是从这些有限的碎片中理解尽可能多的背景信息，捕捉可见部分的丰富视觉表现。\n",
    "MAE解码器： 编码器完成工作后，解码器开始猜测缺失的补丁应该是什么样子，完全根据编码器的见解重建图像。解码器通常更简单、更快，专为重建这些缺失的补丁而设计。\n",
    "视觉对比学习 - CLIP\n",
    "图像编码器：可以将其视为 CLIP 的眼睛 。它获取图像，将其分解为视觉特征（图片中的重要图案或元素），然后将其以数字形式表示。此表示称为图像嵌入 。\n",
    "文本编码器：可以将其视为 CLIP 的大脑 ，但针对的是文字。它读取字幕并将其转换为称为文本嵌入的数字表示。\n",
    "\n",
    "图像生成-扩散模型\n",
    "\n",
    "![扩散模型与VAE/GAN对比](image-26.png)\n",
    "\n",
    "扩散模型如何工作的？\n",
    "正向扩散 - 噪声添加\n",
    "从一张原始的高质量图像开始（例如，阳光照耀的田野中一朵鲜艳的向日葵），想象通过在每个步骤中注入少量高斯噪声来系统地破坏它。每次增量添加都会使图像略微模糊并抹去精细细节。重复此过程数百次甚至数千次，原始图像逐渐失去其结构，直到完全变成随机噪声——与充满静电的屏幕难以区分。\n",
    "\n",
    "![正向扩散过程](image-27.png)\n",
    "\n",
    "反向扩散 - 去噪阶段\n",
    "在此阶段，模型学习反转噪声添加过程。在训练过程中，复杂的神经网络（通常基于 U-Net 架构构建）会进行优化，以预测和减去每一步的噪声。想象一下，从一张几乎完全嘈杂的图像开始；网络通过估计在每次添加少量噪声之前图像的样子来迭代地对其进行细化。通过这种迭代清理，模型逐渐从混乱中重建出连贯而详细的图像。\n",
    "\n",
    "![反向扩散阶段](image-28.png)\n",
    "\n",
    "Diffusion Transformers（DiT）\n",
    "补丁化和标记化：将潜在图像分割成固定大小的补丁。每个补丁线性嵌入到标记中，并添加位置嵌入以保持空间上下文。\n",
    "基于Transformer的处理：标记通过多个 Transformer 块传递，这些块应用多头自注意力来捕获局部和全局依赖关系。条件信息（如噪声水平或类标签）通过交叉注意力或自适应层规范化等机制纳入，使模型能够有效地指导去噪过程。\n",
    "迭代去噪：训练网络逐步预测潜在表示并从潜在表示中减去噪声。经过多次迭代，噪声潜在表示逐渐细化为连贯的高质量图像，然后由 VAE 的解码器对其进行解码。\n",
    "\n",
    "潜在扩散模型（LDM）\n",
    "LDM 不是直接在全像素空间中执行扩散过程，而是在低维潜在空间中工作。此潜在空间由编码器网络创建，该网络将高分辨率图像压缩为紧凑表示 - 一种保留基本视觉信息同时丢弃冗余细节的高效代码。通过在这个缩小的空间中操作，计算负荷显著降低，从而能够在不影响质量的情况下更快地生成图像。\n",
    "高分辨率图像通过编码器，编码器将其转换为潜在表示。\n",
    "使用诸如 U-Net 或扩散变换器之类的去噪网络将扩散过程（包括前向噪声添加和反向去噪）应用于此潜在代码。\n",
    "解码器网络将细化的潜在表示重建为全分辨率、视觉丰富的图像。\n",
    "\n",
    "音频生成\n",
    "音频信号的特征提取\n",
    "    频谱图\n",
    "    梅尔频率倒谱系数\n",
    "    色度特征\n",
    "预处理\n",
    "\n",
    "AudioCraft\n",
    "MusicGen： 专注于根据文本提示生成音乐。\n",
    "AudioGen： 用于创建一般音频效果（例如环境声音、脚步声等）。\n",
    "EnCodec： 用于高效音频压缩和生成的神经音频编解码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与人工智能交互\n",
    "n-shotprompting\n",
    "chain-of-thought prompting（思路链提示）\n",
    "role prompting（角色提示）\n",
    "negative prompting（负面提示）\n",
    "\n",
    "# RAG\n",
    " 检索器：快速扫描文档集合（可以是从一组网页到内部 Wiki 或知识库的任何内容），并返回与用户查询最相关的段落。\n",
    " 生成器 （通常是 LLM）：通过将其所知与刚刚检索到的文本相结合来制作连贯的响应。\n",
    " 在典型的 RAG 管道中，用户查询首先进入检索器 ，检索器根据相关性对文档进行评分和排序。接下来，排名靠前的文档会以某种结构化格式附加或添加到查询中，例如“上下文：[检索到的段落]。问题：[用户查询]”。然后， 生成器会处理此提示，将检索到的内容编织到其最终输出中。\n",
    " ![RAG工作原理](image-29.png)\n",
    "\n",
    "# 实际案例\n",
    "![GenAI开发过程](image-30.png)\n",
    "\n",
    "## 文本到文本生成系统\n",
    "![文本到文本生成系统高级设计]](image-31.png)\n",
    "|阶段 |目的|\n",
    "|---------- |-------------------------|\n",
    "|数据管道和预处理 |初始阶段是收集、清理、标准化和组织原始数据，为人工智能模型创建高质量的训练材料。\n",
    "|模型架构训练     |在学习阶段，模型使用特定的人工智能技术和模式进行训练，以理解和生成类似人类的文本。\n",
    "|推理管道        |在执行阶段，训练有素的系统处理用户输入并实时生成适当的响应。\n",
    "|系统架构和部署   |实施阶段侧重于建立基础设施，使人工智能系统可用、可扩展、可靠，可供实际使用。\n",
    "\n",
    "### 数据管道和预处理\n",
    "文本清理和规范化\n",
    "标记化策略\n",
    "创建训练数据集\n",
    "数据质量和过滤机制\n",
    "![数据管道和预处理](image-32.png)\n",
    "### 模型架构训练\n",
    "Transformer架构\n",
    "嵌入技术：语言模型可以采用从广泛的 BPE 词汇表中学习到的标记嵌入，并结合旋转位置嵌入 (RoPE)。\n",
    "训练目标\n",
    "调优策略\n",
    "模型评估指标\n",
    "### 推理管道\n",
    "提示工程与格式化\n",
    "采样策略\n",
    "响应生成和过滤\n",
    "上下文窗口管理\n",
    "安全过滤器和内容审核\n",
    "![推理流程](image-33.png)\n",
    "### 系统架构和部署\n",
    "API设计和请求处理\n",
    "负载平衡和扩展策略\n",
    "模型服务基础设施\n",
    "监控和日志记录\n",
    "测试框架\n",
    "![系统架构](image-34.png)\n",
    "\n",
    "## 文本到图像生成系统\n",
    "视觉解读引擎： 分析客户的描述，分解艺术元素，将抽象概念转化为精准的技术指令。它还执行关键的安全检查，并确保所有请求符合系统的功能和准则。\n",
    "图像创建核心： 这是真正产生魔力的地方。它使用先进的人工智能技术，逐步从头开始构建图像，通过数千次细微调整对其进行改进，直到它们符合客户的意图。该系统维护多个协同工作的专用神经网络，每个网络都专注于图像创建的不同方面。\n",
    "技术协调器： 此服务可同时处理大量创建请求，并在需要时分配计算能力。它还管理系统资源，确保每个图像生成过程顺利运行，不会干扰其他过程。如果出现任何技术问题，它会迅速解决，以保持不间断的服务。\n",
    "![高级设计](image-35.png)\n",
    "### 数据管道和预处理\n",
    "文本-图形对收集\n",
    "字幕处理\n",
    "图像预处理\n",
    "数据质量和过滤机制\n",
    "![数据处理流程](image-36.png)\n",
    "### 模型架构训练\n",
    "模型架构：基于扩散和自回归的方法\n",
    "文本编码\n",
    "训练目标\n",
    "微调策略\n",
    "模型评估：扩散模型通常使用 FID 和 CLIP 分数来评估图像质量和文本对齐。自回归模型主要使用基于可能性的指标，如 NLL 和 BPD 。\n",
    "![训练过程](image-37.png)\n",
    "### 推理管道\n",
    "提示处理和优化\n",
    "生成过程控制（模型）\n",
    "后期处理和增强\n",
    "安全过滤器和内容审核\n",
    "![推理过程](image-38.png)\n",
    "### 系统架构和部署\n",
    "API设计和请求处理\n",
    "负载平衡和扩展策略\n",
    "模型服务基础设施\n",
    "监控和日志记录\n",
    "输出处理器\n",
    "![系统架构](image-39.png)\n",
    "\n",
    "## 文本转语音生成系统\n",
    "![生成流程](image-40.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
